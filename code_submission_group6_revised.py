# -*- coding: utf-8 -*-
"""Untitled72.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vjf7qPSY30cJbyUn7W47rivO8cJcIvZ9
"""

!pip install gradio

import requests
import time
import json
import pandas as pd
import os
from openai import OpenAI
from pydantic import BaseModel
from openai import OpenAI
from typing import Literal


BASE_URL = "https://world-engine-team6.becknprotocol.io"
STRAPI_API_URL = "https://world-engine-team6.becknprotocol.io"
API_TOKEN = STRAPI_T  # Replace with your actual token
STRAPI_HEADERS = {
    "Authorization": f"Bearer {API_TOKEN}",
    "Content-Type": "application/json"
}
os.environ["OPENAI_API_KEY"] = "Your_key_here"
client = OpenAI()

# === CONFIG ===
BASE_URL = "https://world-engine-team6.becknprotocol.io"
JWT = API_TOKEN  # Replace with your actual token
HEADERS = {
    "Authorization": f"Bearer {JWT}",
    "Content-Type": "application/json"
}
ENDPOINT = "ders"

# === Fetch DER data ===
def poll_strapi(endpoint):
    url = f"{BASE_URL}/api/{endpoint}?publicationState=preview&pagination[pageSize]=100"
    print(f"üì° Requesting: {url}")

    try:
        res = requests.get(url, headers=HEADERS)
        print(f"‚úÖ Status Code: {res.status_code}")
        res.raise_for_status()
        return res.json()
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Network or Auth Error: {e}")
    except ValueError:
        print(f"‚ùå Invalid JSON. Raw content:\n{res.text[:300]}")

# === Group DERs by User ===
def get_user_der_map(ders_response):
    user_map = {}

    for der in ders_response.get("data", []):
        der_id = der.get("id")
        der_attrs = der.get("attributes", {})
        appliance = der_attrs.get("appliance", {}).get("data", {})

        if not appliance:
            continue

        appliance_attrs = appliance.get("attributes", {})
        user_id = appliance_attrs.get("user_id", "unknown")
        appliance_name = appliance_attrs.get("name", "unknown")
        transformer = appliance_attrs.get("transformer_id", "unknown")

        if user_id not in user_map:
            user_map[user_id] = []

        user_map[user_id].append({
            "der_id": der_id,
            "appliance": appliance_name,
            "transformer": transformer,
            "switched_on": der_attrs.get("switched_on", None)
        })

    return user_map

# === Run Example ===
if __name__ == "__main__":
    ders_data = poll_strapi(ENDPOINT)
    if not ders_data:
        exit()

    user_ders = get_user_der_map(ders_data)

    print("\nüîç DERs grouped by user:")
    for user, ders in user_ders.items():
        print(f"\nüë§ User {user}")
        for der in ders:
            print(f"  ‚Ä¢ DER ID {der['der_id']}, {der['appliance']} on {der['transformer']}, switched_on={der['switched_on']}")

build_user_der_summary()

url = f"{BASE_URL}/api/{ENDPOINT}?publicationState=preview&pagination[pageSize]=100"
res = requests.get(url, headers=HEADERS)
print(f"Status: {res.status_code}")
print(res.headers["Content-Type"])
print(res.text[:300])
for der in res.json()["data"]:
    print(json.dumps(der, indent=2))
    break

import requests
import json
import pandas as pd
from datetime import datetime, timedelta, timezone

BASE_URL = "https://world-engine-team6.becknprotocol.io"

HEADERS = {
    "Authorization": f"Bearer {API_TOKEN}",
    "Content-Type": "application/json"
}

def fetch(endpoint):
    url = f"{BASE_URL}/api/{endpoint}?populate=*&pagination[pageSize]=100"
    res = requests.get(url, headers=HEADERS)
    res.raise_for_status()
    return res.json()["data"]

def get_meter_user_map(meters):
    id_to_user = {}
    for m in meters:
        mid = m["id"]
        user = (
            m["attributes"]
            .get("energyResource", {})
            .get("data", {})
            .get("attributes", {})
            .get("name", "unknown_user")
        )
        id_to_user[mid] = user
    return id_to_user

def compute_last_hour_usage(datasets, id_to_user):
    usage_map = {}
    cutoff = datetime.now(timezone.utc) - timedelta(hours=1)

    for record in datasets:
        attrs = record["attributes"]
        timestamp = attrs.get("timestamp")
        if not timestamp:
            continue

        ts_dt = datetime.fromisoformat(timestamp.replace("Z", "+00:00"))
        if ts_dt < cutoff:
            continue

        meter = attrs.get("meter", {}).get("data", {})
        if not meter:
            continue

        mid = meter["id"]
        user = id_to_user.get(mid, "unknown_user")
        usage = attrs.get("consumptionKWh", 0)

        if user not in usage_map:
            usage_map[user] = 0
        usage_map[user] += usage

    return usage_map

def map_user_to_ders(ders):
    user_der_map = {}

    for der in ders:
        der_id = str(der["id"])
        appliance = der["attributes"].get("appliance", {}).get("data", {})
        if not appliance:
            continue

        appliance_attrs = appliance.get("attributes", {})
        user = appliance_attrs.get("user_id", "unknown_user")
        appliance_type = appliance_attrs.get("name", "unknown_appliance")

        if user not in user_der_map:
            user_der_map[user] = {"der_ids": [], "types": []}

        user_der_map[user]["der_ids"].append(der_id)
        user_der_map[user]["types"].append(appliance_type)

    return user_der_map

# === MAIN ===
meters = fetch("meters")
datasets = fetch("meter-datasets")
ders = fetch("ders")

id_to_user = get_meter_user_map(meters)
usage_last_hour = compute_last_hour_usage(datasets, id_to_user)
user_der_map = map_user_to_ders(ders)

# Join usage with DER info
rows = []
for user, usage in usage_last_hour.items():
    der_info = user_der_map.get(user, {"der_ids": [], "types": []})
    rows.append({
        "user": user,
        "kWh_last_hour": round(usage, 5),
        "der_ids": der_info["der_ids"],
        "der_types": der_info["types"]
    })

df = pd.DataFrame(rows).sort_values(by="kWh_last_hour", ascending=False)

print("üîã Consumption in the Last Hour + DER Info:")
print(df.to_markdown(index=False))

def map_user_to_ders(ders):
    user_der_map = {}

    for der in ders:
        der_id = str(der["id"])

        # Get appliance name/type
        appliance = der["attributes"].get("appliance", {}).get("data", {})
        appliance_name = appliance.get("attributes", {}).get("name", "unknown_appliance")

        # Get user from energy_resource
        user = (
            der["attributes"]
            .get("energy_resource", {})
            .get("data", {})
            .get("attributes", {})
            .get("name", "unknown_user")
        )

        if user not in user_der_map:
            user_der_map[user] = {"der_ids": [], "types": []}

        user_der_map[user]["der_ids"].append(der_id)
        user_der_map[user]["types"].append(appliance_name)

    return user_der_map

import requests
import json
import pandas as pd
from datetime import datetime, timedelta, timezone

# === Config ===
BASE_URL = "https://world-engine-team6.becknprotocol.io"

HEADERS = {
    "Authorization": f"Bearer {API_TOKEN}",
    "Content-Type": "application/json"
}

def fetch(endpoint):
    url = f"{BASE_URL}/api/{endpoint}?populate=*&pagination[pageSize]=100"
    res = requests.get(url, headers=HEADERS)
    res.raise_for_status()
    return res.json()["data"]

def filter_meters_by_transformers(meters, transformer_names):
    meter_user_map = {}
    for m in meters:
        meter_id = m["id"]
        attrs = m["attributes"]
        transformer = attrs.get("transformer", {}).get("data", {})
        tf_name = transformer.get("attributes", {}).get("name", None)

        if tf_name in transformer_names:
            energy_resource = attrs.get("energyResource", {}).get("data", {})
            user = energy_resource.get("attributes", {}).get("name", "unknown_user")
            if user:
                meter_user_map[meter_id] = user
    return meter_user_map

def map_user_to_ders(ders):
    user_der_map = {}
    for der in ders:
        der_id = str(der["id"])
        appliance = der["attributes"].get("appliance", {}).get("data", {})
        appliance_name = appliance.get("attributes", {}).get("name", "unknown_appliance")
        user = (
            der["attributes"]
            .get("energy_resource", {})
            .get("data", {})
            .get("attributes", {})
            .get("name", "unknown_user")
        )

        if user not in user_der_map:
            user_der_map[user] = {"der_ids": [], "types": []}

        user_der_map[user]["der_ids"].append(der_id)
        user_der_map[user]["types"].append(appliance_name)
    return user_der_map

def compute_user_usage_last_hour(datasets, meter_user_map):
    usage_by_user = {}
    cutoff = datetime.now(timezone.utc) - timedelta(hours=1)

    for record in datasets:
        attrs = record["attributes"]
        ts_str = attrs.get("timestamp")
        if not ts_str:
            continue
        ts = datetime.fromisoformat(ts_str.replace("Z", "+00:00"))
        if ts < cutoff:
            continue

        meter = attrs.get("meter", {}).get("data", {})
        meter_id = meter.get("id")
        if meter_id not in meter_user_map:
            continue

        user = meter_user_map[meter_id]
        kwh = attrs.get("consumptionKWh", 0)
        usage_by_user[user] = usage_by_user.get(user, 0) + kwh

    return usage_by_user

# === Main Pipeline ===
def get_users_ders_and_usage(transformer_names):
    print(f"üîç Filtering for transformers: {transformer_names}")
    meters = fetch("meters")
    ders = fetch("ders")
    datasets = fetch("meter-datasets")

    meter_user_map = filter_meters_by_transformers(meters, transformer_names)
    user_der_map = map_user_to_ders(ders)
    user_kwh = compute_user_usage_last_hour(datasets, meter_user_map)

    rows = []
    for user, kwh in user_kwh.items():
        if user in user_der_map:
            rows.append({
                "user": user,
                "kWh_last_hour": round(kwh, 5),
                "der_ids": user_der_map[user]["der_ids"],
                "der_types": user_der_map[user]["types"]
            })

    if not rows:
        print("‚ö†Ô∏è No users with DERs found for given transformers.")
        return pd.DataFrame()

    df = pd.DataFrame(rows).sort_values(by="kWh_last_hour", ascending=False)
    return df

# === Example Run ===
transformers_to_filter = ["Transformer_1", "Transformer_3", "Transformer_5"]
df_result = get_users_ders_and_usage(transformers_to_filter)

# ‚úÖ Display
print("\n‚ö° Users with DERs and Consumption in Last Hour:")
print(df_result.to_markdown(index=False))

import random
import pandas as pd

# Constants
TRANSFORMERS = [f"Transformer_{i+1}" for i in range(10)]
MAX_CAPACITY = 10

def simulate_forecast_run():
    temperature = random.uniform(25, 42)  # degrees Celsius
    humidity = random.uniform(40, 90)     # percentage

    forecast = {}
    for tf in TRANSFORMERS:
        if temperature > 35 or humidity > 75:
            overload_prob = 0.6
        else:
            overload_prob = 0.3

        if random.random() < overload_prob:
            load = round(random.uniform(9.5, 12), 2)  # Over capacity
        else:
            load = round(random.uniform(5, 9.5), 2)   # Within safe range

        forecast[tf] = load

    forecast["Temperature"] = round(temperature, 1)
    forecast["Humidity"] = round(humidity, 1)
    return forecast

# Run simulation 10 times
forecast_runs = [simulate_forecast_run() for _ in range(10)]

# Convert to DataFrame
df = pd.DataFrame(forecast_runs)

# Display
import matplotlib.pyplot as plt
import matplotlib.ticker as mtick
from IPython.display import display
print("üîÆ Forecasted Transformer Loads (10 Runs):")
display(df)

# Optional: heatmap-like visualization
df_heatmap = df[TRANSFORMERS]
df_heatmap.plot(kind="bar", figsize=(15, 5), title="Transformer Load Forecasts")
plt.axhline(MAX_CAPACITY, color='red', linestyle='--', label='Capacity (10 kWh)')
plt.ylabel("Forecast Load (kWh)")
plt.legend()
plt.tight_layout()
plt.show()

response = client.responses.create(model="gpt-4.1",temperature=0.5,
instruction=
                    "You are an agent that looks at weather forecast dat"
                                  input="hi")
print(response.output_text)

print(df)

import random
import pandas as pd

# === Simulate 20 forecast windows for 10 transformers ===
TRANSFORMERS = [f"Transformer_{i+1}" for i in range(10)]
MAX_CAPACITY = 10
FORECAST_THRESHOLD = 8  # threshold to decide overload

def simulate_forecast_batch(n_rows=20):
    rows = []
    for _ in range(n_rows):
        temperature = round(random.uniform(25, 42), 1)
        humidity = round(random.uniform(40, 90), 1)
        row = {}
        total_load = 0
        for tf in TRANSFORMERS:
            overload_prob = 0.6 if (temperature > 35 or humidity > 75) else 0.3
            load = round(random.uniform(9.5, 12), 2) if random.random() < overload_prob else round(random.uniform(5, 9.5), 2)
            row[tf] = load
            total_load += load
        row["Total"] = round(total_load, 2)
        row["Temperature"] = temperature
        row["Humidity"] = humidity
        rows.append(row)
    df_forecast = pd.DataFrame(rows)
    return df_forecast

# Generate 20 forecast snapshots
forecast_df = simulate_forecast_batch(20)
forecast_df.index = [f"Window_{i+1}" for i in range(20)]

print(forecast_df.iloc[0].to_markdown())


openai = OpenAI()

# Pydantic model for the structured output
class ForecastEval(BaseModel):
    call_rl_agent: bool
    reasoning_trace: str
    message_to_rl: str

# Extract first row from the forecast DataFrame
first_forecast_row = forecast_df.iloc[0]
current_forecast_markdown = first_forecast_row.to_frame().T.to_markdown(index=False)

# Prompt instruction
instructions = """
You are a grid monitoring assistant. Given forecasted transformer loads for the next interval,
decide if the system needs to trigger the RL agent to shed load.

### Mandatory Rules:
- If any transformer load exceeds 9.0, you MUST call the RL agent.
- If the TOTAL load exceeds 80.0, you MUST call the RL agent.
- If many of the transformers are near 8.5‚Äì8.9, you MAY choose to call the RL agent proactively.
- If loads are mostly around 6‚Äì7, then it is safe to not call the RL agent yet.

### Your Output:
- `call_rl_agent`: True or False
- `reasoning_trace`: Explain why you decided to trigger or not trigger the RL agent.
- `message_to_rl`: If `call_rl_agent` is true, write a message describing why RL agent is being called and what it should look for.
"""

# Call the LLM using the Responses API
forecast_eval_response = openai.responses.parse(
    model="gpt-4.1",
    instructions=instructions,
    input=current_forecast_markdown,
    text_format=ForecastEval
)

# Parse with Pydantic
forecast_eval_result = forecast_eval_response.output_text
forecast_eval_result

import requests

jwt = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6MSwiaWF0IjoxNzQ3MjE2MTMwLCJleHAiOjE3NDk4MDgxMzB9.dCcPst8kjWqfGFbLjN7yGrypoQpTGwCb4Bicw8sv16E"
import requests
import json
from urllib.parse import urljoin
import requests
import json

BASE_URL = f"https://world-engine-team6.becknprotocol.io/api/ders?populate[appliance]=*&populate[energy_resource]=*"
JWT = "685bb03f74c3fa3d4c4772f951308e0ff3ee8a21af4669ad9137f215a2c4a3f498514bf93db64c90417674522487c4d54ed67dbc06ac7a21c37ae4a44d8bdb1a360158734c6fa035a39f5bbc34da8b1165cbaca6ac16c3d9d918a9632db59d58165755c74dc4e8c3b2d7482e9dad56ba13ecd6019b26f4921d02932af0b27346"
HEADERS = {
    "Authorization": f"Bearer {JWT}",
    "Content-Type": "application/json"
}

endpoints = [
    "appliances",
    "ders",
    "energy-resources",
    "grid-loads",
    "meters",
    "meter-datasets",
    "p2p-trades",
    "substations",
    "transformers",
    "utilities"
]

for ep in endpoints:
    url = f"{BASE_URL}/api/{ep}?publicationState=preview&pagination[pageSize]=100"
    print(f"üì• Fetching {url}")
    res = requests.get(url, headers=HEADERS)

    if res.status_code == 200:
        data = res.json()
        with open(f"{ep}.json", "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2)
        print(f"‚úÖ Saved {ep}.json")
    else:
        print(f"‚ùå Failed to fetch {ep}: {res.status_code}")
        print(res.text)

# Step 2: Load and combine the energy + appliance data
import json
import pandas as pd
import random
import numpy as np

# Load JSON files
energy_data = json.load(open("energy-resources.json"))
appliance_data = json.load(open("appliances.json"))
combined_data = energy_data["data"] + appliance_data["data"]

# Step 3: Extract user-appliance energy records
records = []
for entry in combined_data:
    try:
        attrs = entry["attributes"]
        user = attrs["energy_resource"]["data"]["attributes"]["name"]
        appliance = attrs["appliance"]["data"]["attributes"]["name"]
        der_code = entry["id"]
        base_kwh = float(attrs["appliance"]["data"]["attributes"].get("baseKWh", 0))
        records.append({
            "User": user,
            "Appliance": appliance,
            "DER Code": der_code,
            "Base kWh Usage": base_kwh
        })
    except Exception:
        continue

df = pd.DataFrame(records)

# Step 4: Assign users to 4 transformers evenly
unique_users = df["User"].unique().tolist()
random.shuffle(unique_users)
transformers = [f"Transformer_{i+1}" for i in range(4)]
user_to_transformer = {user: transformers[i % 4] for i, user in enumerate(unique_users)}
df["Assigned Transformer"] = df["User"].map(user_to_transformer)

# Step 5: Aggregate transformer summary
summary_df = df.groupby("Assigned Transformer").agg(
    Total_Users=("User", "nunique"),
    Total_Energy_Output_kWh=("Base kWh Usage", "sum")
).reset_index()

# Add TOTAL row
total_row = summary_df[["Total_Users", "Total_Energy_Output_kWh"]].sum().to_frame().T
total_row["Assigned Transformer"] = "TOTAL"
summary_with_total = pd.concat([summary_df, total_row], ignore_index=True)

# Step 6: Forecast generation for each transformer
np.random.seed(42)
transformer_ids = summary_df["Assigned Transformer"].tolist()
forecast_records = []

for tx in transformer_ids:
    base_output = summary_df.loc[summary_df["Assigned Transformer"] == tx, "Total_Energy_Output_kWh"].values[0]
    for t in range(20):
        temperature = np.random.normal(loc=30, scale=3)
        humidity = np.random.normal(loc=60, scale=10)
        forecast_kWh = base_output * (1 + 0.01 * (temperature - 30) - 0.005 * (humidity - 60)) + np.random.normal(0, 0.01)
        forecast_records.append({
            "Transformer": tx,
            "Reading": t + 1,
            "Forecasted Energy (kWh)": round(forecast_kWh, 4)
        })

# Step 7: Pivot table ‚Äî rows = readings, columns = transformers
forecast_df = pd.DataFrame(forecast_records)
pivot_df = forecast_df.pivot(index="Reading", columns="Transformer", values="Forecasted Energy (kWh)").reset_index()

# Step 8: Add total forecasted energy column
pivot_df["Total Forecasted Energy (kWh)"] = pivot_df[
    [col for col in pivot_df.columns if col.startswith("Transformer_")]
].sum(axis=1)

# Optional: round for clarity
pivot_df = pivot_df.round(4)

# Step 9: Display final DataFrame
pivot_df

import gradio as gr
from pydantic import BaseModel
from openai import OpenAI
import pandas as pd

# Assume forecast_df is already available
# Make sure forecast_df contains: 'Transformer_1', 'Transformer_2', ..., 'Total Forecasted Energy (kWh)'

openai = OpenAI()
forecast_df = pivot_df
class ForecastEval(BaseModel):
    call_rl_agent: bool
    reasoning_trace: str
    message_to_rl: str

forecast_index = 0
reasoning_log = []

def run_next_window():
    global forecast_index, reasoning_log
    if forecast_index >= len(forecast_df):
        return "‚úÖ All windows evaluated.", ""

    current_row = forecast_df.iloc[forecast_index]
    short_df = current_row.to_frame().T[
        [col for col in forecast_df.columns if col.startswith("Transformer_")] + ["Total Forecasted Energy (kWh)"]
    ]
    markdown_table = short_df.to_markdown(index=False)

    forecast_eval_response = openai.responses.parse(
        model="gpt-4.1",
        input=markdown_table,
        text_format=ForecastEval,
        instructions="""
You are a grid assistant evaluating transformer load forecasts.

### RULES:
- If **ANY** transformer > 1.1 ‚Üí must call RL agent
- If **Total Forecasted Energy** > 3.25 ‚Üí must call RL agent
- If values are **close** (e.g., any transformer > 1.07, total > 3.2) ‚Üí MAY call RL agent
- Else ‚Üí no action

### OUTPUT:
- call_rl_agent: True or False
- reasoning_trace: Explain why agent was/would not be called
- message_to_rl: If agent called, give a message like "Overload risk in Tx 2" or "Total load nearing capacity"
"""
    )

    parsed = forecast_eval_response.output_parsed
    log = f"""
### Forecast Window {forecast_index+1}
{markdown_table}

**Call RL Agent**: {parsed.call_rl_agent}

**Reasoning Trace**:
{parsed.reasoning_trace}

**Message to RL Agent**:
{parsed.message_to_rl}
"""
    forecast_index += 1
    reasoning_log.append(log)
    return "\n---\n".join(reasoning_log), ""

with gr.Blocks() as demo:
    gr.Markdown("## üîç Forecast Evaluation and RL Trigger Agent")
    log_view = gr.Markdown()

    btn = gr.Button("‚ñ∂Ô∏è Run Next Forecast")
    btn.click(fn=run_next_window, outputs=[log_view, gr.Textbox(visible=False)])

demo.launch(debug=True)

from openai import OpenAI
import pandas as pd
import json
from pydantic import BaseModel
from typing import List, Dict

client = OpenAI()

formatted_df = pivot_df.copy()
# ‚úÖ Your dataframe must already be defined
# df must have columns: 'Assigned Transformer', 'User', 'Appliance'

# ‚úÖ Step 1: Tool function
def get_tx_appliance_map(transformer_ids: List[str]) -> Dict[str, Dict[str, List[str]]]:
    filtered_df = df[df["Assigned Transformer"].isin(transformer_ids)]
    result = {}
    for tx in transformer_ids:
        tx_df = filtered_df[filtered_df["Assigned Transformer"] == tx]
        users = tx_df["User"].unique().tolist()
        result[tx] = {
            user: tx_df[tx_df["User"] == user]["Appliance"].tolist()
            for user in users
        }
    return result

# ‚úÖ Step 2: Schema for first LLM
class ForecastEval(BaseModel):
    call_rl_agent: bool
    reasoning_trace: str
    message_to_rl: str

# ‚úÖ Step 3: Tool schema
tools = [{
    "type": "function",
    "name": "get_tx_appliance_map",   # üëà This MUST be at the top-level
    "description": "Returns users and appliances for transformers",
    "parameters": {
        "type": "object",
        "properties": {
            "transformer_ids": {
                "type": "array",
                "items": {"type": "string"},
                "description": "List of transformer IDs like ['Transformer_1']"
            }
        },
        "required": ["transformer_ids"]
    }
}]

# ‚úÖ Step 4: Forecast evaluation logic (responses.parse + Pydantic schema)
def evaluate_forecast(row_dict):
    table = pd.DataFrame([row_dict]).to_markdown(index=False)

    response = client.responses.parse(
        model="gpt-4.1",
        input=table,
        text_format=ForecastEval,
        instructions="""
You are a grid assistant evaluating transformer load forecasts.

RULES:
- If ANY transformer > 1.10 ‚Üí must call RL agent
- If total > 3.25 ‚Üí must call RL agent
- If close (e.g. transformer > 1.05 or total > 3.1) ‚Üí may call RL agent

OUTPUT:
- call_rl_agent: True/False
- reasoning_trace: explain clearly
- message_to_rl: what you want the RL agent to do
"""
    )

    return response.output_parsed


# ‚úÖ Step 5: Run RL agent + tool call if required

out = evaluate_forecast(forecast_df.iloc[0])
response = client.responses.create(
    model="gpt-4.1",
    input=[{"role": "user", "content": out.message_to_rl}],
    tools=tools,
    tool_choice="auto",
    instructions="""
You are an RL grid agent. You must obey the RL planner message.
Use the `get_tx_appliance_map` tool to get users/appliances for transformers mentioned.
"""
)

tool_calls = response.output[0]

print(tool_calls)
args = json.loads(tool_calls.arguments)




# ‚úÖ Step 6: Example usage
forecast_row = {
    "Transformer_1": 0.95,
    "Transformer_2": 1.14,
    "Transformer_3": 1.03,
    "Transformer_4": 1.12,
    "Total Forecasted Energy (kWh)": 4.24
}

# First LLM
parsed = evaluate_forecast(forecast_row)
print("üîç Forecast Evaluation:\n", parsed)

# If RL action is needed
if parsed.call_rl_agent:
    print("‚ö†Ô∏è Triggering RL agent...\n")
    tool_result = run_rl_tool_agent(parsed.message_to_rl)
    print("üîß Tool Result:\n", tool_result)
else:
    print("‚úÖ No RL action needed.")

import pandas as pd
import json
from pydantic import BaseModel
from typing import List, Dict
from openai import OpenAI

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Prerequisites:
#   pip install --upgrade openai pydantic pandas
#   Ensure you have `formatted_df` and `df` defined as described.
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

client = OpenAI()

# --- Step 1: Forecast Evaluation Schema & Call ---
class ForecastEval(BaseModel):
    call_rl_agent: bool
    reasoning_trace: str
    message_to_rl: str

# Take the first row of your pivoted forecast DataFrame:
first_row = formatted_df.iloc[0].to_dict()
row_md = pd.DataFrame([first_row]).to_markdown(index=False)

resp1 = client.responses.parse(
    model="gpt-4.1",
    input=[{"role": "user", "content": row_md}],
    text_format=ForecastEval,
    instructions="""
You are a transformer load evaluation assistant.

Rules:
- If any transformer > 1.10 ‚Üí must call RL agent
- If total > 3.25 ‚Üí must call RL agent
- If close (>1.05 or total>3.1) ‚Üí may call RL agent

Return:
call_rl_agent: bool
reasoning_trace: str
message_to_rl: str
"""
)
parsed1 = resp1.output_parsed
print("ForecastEval:", parsed1)

# --- Step 2: Define the Shedding Tool Function ---
def get_tx_appliance_map(transformer_ids: List[str]) -> Dict[str, Dict[str, List[Dict[str, float]]]]:
    filtered = df[df["Assigned Transformer"].isin(transformer_ids)]
    out = {}
    for tx in transformer_ids:
        tx_df = filtered[filtered["Assigned Transformer"] == tx]
        users = tx_df["User"].unique()
        user_map = {}
        for user in users:
            user_df = tx_df[tx_df["User"] == user]
            appliances = [
                {
                    "name": row["Appliance"],
                    "usage_kWh": round(row.get("Base kWh Usage", 0), 4)
                }
                for _, row in user_df.iterrows()
            ]
            user_map[user] = appliances
        out[tx] = user_map
    return out


# Define the tool schema at top level:
tools = [{
    "type": "function",
    "name": "get_tx_appliance_map",
    "description": "Returns users and appliances for given transformer IDs.",
    "parameters": {
        "type": "object",
        "properties": {
            "transformer_ids": {
                "type": "array",
                "items": {"type": "string"},
                "description": "List of transformer IDs"
            }
        },
        "required": ["transformer_ids"]
    }
}]

# --- Step 3: Invoke the Tool Agent if Needed ---
tx_map = {}
if parsed1.call_rl_agent:
    resp2 = client.responses.create(
        model="gpt-4.1",
        input=[{"role": "user", "content": parsed1.message_to_rl + "\n" + row_md}],
        tools=tools,
        tool_choice="auto",
        instructions="""
You are an RL grid agent. You will get a message request from the forecaster to suggest which transformers to inspect, You have to choose from Transformer_1 upto Transformer_10 ,please follow this format.Rules:
- If any transformer > 1.10 ‚Üí must call RL agent
- If total > 3.25 ‚Üí must call RL agent
- If close (>1.05 or total>3.1) ‚Üí may call RL agent

Return:
call_rl_agent: bool
reasoning_trace: str
message_to_rl: str
"""


    )
    call = resp2.output[0]
    #args = json.loads()
    print(call)
    tx_map = get_tx_appliance_map(args["transformer_ids"])

# --- Step 4: Convert the Map to Markdown ---

def format_tx_map_md(mapping):
    md = "# üîß Transformer Load Shedding Map\n"
    for tx, users in mapping.items():
        md += f"\n## üîå {tx}\n"
        for u, apps in users.items():
            app_lines = [f"{app['name']} ({app['usage_kWh']} kWh)" for app in apps]
            md += f"- **{u}** ‚Üí {', '.join(app_lines)}\n"
    return md

md_map = format_tx_map_md(tx_map)
print(md_map)

formatted_df = pivot_df.copy()
forecast_prompt = """
You are a load forecasting evaluation assistant responsible for assessing transformer energy forecasts in a smart grid.

Your job is to decide whether to escalate to the RL system based on the following rules:

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
üîç Escalation Rules:
- If **any transformer** has a forecast > 1.10 kWh ‚Üí **must escalate** (return `call_rl_agent = true`)
- If **total forecast energy** > 3.25 kWh ‚Üí **must escalate**
- If any transformer > 1.05 or total > 3.1 ‚Üí **may escalate**
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Your output must include:
- `call_rl_agent`: `true` if RL intervention is needed, else `false`
- `reasoning_trace`: A clear explanation of why you made this decision
- `message_to_rl`: A message to send to the RL agent (only required if `call_rl_agent` is `true`). Describe which transformers are overloaded and why load shedding may be needed.

If escalation is not required, return `call_rl_agent = false`, and you may leave the message field empty or indicate that RL intervention is not needed.

You will be given as input:
- Forecasted transformer energy usage for 10 transformers
- The total energy forecast across all transformers

Use calm, critical reasoning. Do not panic or escalate unless justified.
"""

rl_prompt = """
You are an RL load-shedding control agent.

You have received:
- The forecasted energy usage for 10 transformers
- The total forecast energy
- A message and reasoning from a previous evaluation agent indicating potential overload

Your job is to determine **which transformers to investigate** in detail. Do not pick unecessarily only when needed.


Rules (same as evaluation agent):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
- If transformer forecast > 1.10 ‚Üí must include that transformer
- If total > 3.25 ‚Üí include any transformer above 1.05
- Otherwise, include transformers that are **close to threshold** or may become risky
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Always make a function call. Do not unecessarily call a transformer, only call those that are very close(that too only if others are also nearing the danger zone) or if the threshold is exceeded.
"""

final_bot = """
You are a grid load-shedding RL assistant operating within the Beckn protocol.

You have received:
- A list of transformers that are overloaded
- A dictionary of users under each transformer and their associated DER (Distributed Energy Resource) appliances

Your job is to return **structured instructions** to users for load shedding, in a form that complies with the Beckn protocol.

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
üì¶ Your Output Format (one entry per user):
- `user`: Name of the user
- `der`: List of their DER appliances (e.g., ["Geyser", "Microwave"])
- `type`: Action to take, chosen from the allowed Beckn types
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

‚úÖ Allowed `type` values (per Beckn):
- `suggestion`: Soft nudge or advisory
- `request`: Polite ask to shed load
- `command`: Mandatory instruction from the grid (used only when necessary)
- `inform`: No action, just status update

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
üéØ Decision Rules:
- Prefer `suggestion` or `request` for most users
- Use `command` **only** for users contributing significantly to the overload
- **Do not command everyone** ‚Äî be precise and minimal
- You may skip users whose DERs are low impact or not helpful to shed
- Be calm and efficient ‚Äî your goal is **smooth load reduction**, not panic
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Please provide your reasoning clearly stating your choices.

This agent is part of a decentralized grid optimization system. The output will be passed to the Beckn BPP to notify users. Your instructions must be well-balanced, respectful of user comfort, and effective for grid protection.
"""

reflection_bot = """
You are a grid load-shedding RL assistant operating within the Beckn protocol.
This is an improvement step, In the past you had received certain things

You had received:
- A list of transformers that are overloaded
- A dictionary of users under each transformer and their associated DER (Distributed Energy Resource) appliances

Your job is to return **structured instructions** to users for load shedding, in a form that complies with the Beckn protocol.

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
üì¶ Your Output Format (one entry per user):
- `user`: Name of the user
- `der`: List of their DER appliances (e.g., ["Geyser", "Microwave"])
- `type`: Action to take, chosen from the allowed Beckn types
- 'reflection': Based on past logs reflect
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

‚úÖ Allowed `type` values (per Beckn):
- `suggestion`: Soft nudge or advisory
- `request`: Polite ask to shed load
- `command`: Mandatory instruction from the grid (used only when necessary)
- `inform`: No action, just status update

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
üéØ Decision Rules:
- Prefer `suggestion` or `request` for most users
- Use `command` **only** for users contributing significantly to the overload
- **Do not command everyone** ‚Äî be precise and minimal
- You may skip users whose DERs are low impact or not helpful to shed
- Be calm and efficient ‚Äî your goal is **smooth load reduction**, not panic
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Please provide your reasoning clearly stating your choices.

Now what is happening is we will give you past logs, you should reflect on those logs and give a detailed 10 step reflection log first
Were you too conservative(as in made too many strong requests) or were you nolt assertive enough
Tell us what you have learnt.

So in this attempt you will reflect first then reason and make those calls


This agent is part of a decentralized grid optimization system. The output will be passed to the Beckn BPP to notify users. Your instructions must be well-balanced, respectful of user comfort, and effective for grid protection.
"""

from pydantic import Field
# --- Step 1: Forecast Evaluation Schema & Call ---
class ForecastEval(BaseModel):
    call_rl_agent: bool = Field(...,description="Flag indicating whether to call the rl_agent")
    reasoning_trace: str = Field(...,description="Reasoning behind your decisions")
    message_to_rl: str = Field(...,description="Message to the rl_agent asking it take over")

# Take the first rorlw of your pivoted forecast DataFrame:
first_row = formatted_df.iloc[0].to_dict()
row_md = pd.DataFrame([first_row]).to_markdown(index=False)

resp1 = client.responses.parse(
    model="gpt-4.1",
    input=[{"role": "user", "content": row_md}],
    text_format=ForecastEval,
    instructions= forecast_prompt
)
parsed1 = resp1.output_parsed
proceed = parsed1.call_rl_agent
reasoning = parsed1.reasoning_trace
rl_escalation_msg = parsed1.message_to_rl
print(proceed)
print(reasoning)
print(rl_escalation_msg)

resp2 = client.responses.create(
        model="gpt-4.1",
        input = f"**Message from eval agent:**{rl_escalation_msg}\n\n.**Reasoning provided**{reasoning} +**Transformer map**{row_md}",
        tools=tools,
        instructions=rl_prompt,
        tool_choice = {"type": "function", "name": "get_tx_appliance_map"})
call = resp2.output[0]
print(call)
args = json.loads(call.arguments)
print(args)
transformer_map = {str(i): f"Transformer_{i}" for i in range(1, 11)}

val = args["transformer_ids"]  # e.g., ["2", "4", "10"]
print(val)
#transformers = [transformer_map[v] for v in val]
#print(transformers)
tx_map = get_tx_appliance_map(val)

md_map = format_tx_map_md(tx_map)
print(md_map)

# --- Step 5: Next RL Agent for User Actions ---
class UserAction(BaseModel):
    user: List[str] = Field(...,description="List of users that we would like to send a request/suggestion etc")
    der: List[List[str]] = Field(...,description="For each user list their der/appliancesyou would like them to shut")
    message_type: List[str] =  Field(...,description="One of the 4 types, suggestion,request,inform or command for that user")
    reasoning: str = Field(...,description="Explain the reasons behind your decisions")

resp3 = client.responses.parse(
    model="gpt-4.1",
    input= f"**List of users under given transformer and their usage**{md_map}",
    text_format=UserAction,
    instructions = final_bot
)
actions = resp3.output_parsed
print(actions.user)
print(actions.der)
print(actions.message_type)
print(actions.reasoning)

class UserReflection(BaseModel):
    user: List[str] = Field(...,description="List of users that we would like to send a request/suggestion etc")
    der: List[List[str]] = Field(...,description="For each user list their der/appliancesyou would like them to shut")
    message_type: List[str] =  Field(...,description="One of the 4 types, suggestion,request,inform or command for that user")
    reflection: List[str] = Field(...,description="Explain the reasons behind your decisions after thorough reflection atleast 7 steps of reflecting")

from pydantic import Field
# --- Step 1: Forecast Evaluation Schema & Call ---
class ForecastEval(BaseModel):
    call_rl_agent: bool = Field(...,description="Flag indicating whether to call the rl_agent")
    reasoning_trace: str = Field(...,description="Reasoning behind your decisions")
    message_to_rl: str = Field(...,description="Message to the rl_agent asking it take over")

# Take the first rorlw of your pivoted forecast DataFrame:
first_row = formatted_df.iloc[0].to_dict()
row_md = pd.DataFrame([first_row]).to_markdown(index=False)

resp1 = client.responses.parse(
    model="gpt-4.1",
    input=[{"role": "user", "content": row_md}],
    text_format=ForecastEval,
    instructions= forecast_prompt
)
parsed1 = resp1.output_parsed
proceed = parsed1.call_rl_agent
reasoning = parsed1.reasoning_trace
rl_escalation_msg = parsed1.message_to_rl
print(proceed)
print(reasoning)
print(rl_escalation_msg)

resp2 = client.responses.create(
        model="gpt-4.1",
        input = f"**Message from eval agent:**{rl_escalation_msg}\n\n.**Reasoning provided**{reasoning}",
        tools=tools,
        instructions=rl_prompt,
        tool_choice = {"type": "function", "name": "get_tx_appliance_map"})
call = resp2.output[0]
args = json.loads(call.arguments)
transformer_map = {str(i): f"Transformer_{i}" for i in range(1, 11)}

val = args["transformer_ids"]  # e.g., ["2", "4", "10"]
#transformers = [transformer_map[v] for v in val]
transformers=val
print(transformers)
tx_map = get_tx_appliance_map(transformers)
md_map = format_tx_map_md(tx_map)
print(md_map)

# --- Step 5: Next RL Agent for User Actions ---
class UserAction(BaseModel):
    user: List[str] = Field(...,description="List of users that we would like to send a request/suggestion etc")
    der: List[List[str]] = Field(...,description="For each user list their der/appliancesyou would like them to shut")
    message_type: List[str] =  Field(...,description="One of the 4 types, suggestion,request,inform or command for that user")
    reasoning: str = Field(...,description="Explain the reasons behind your decisions")

resp3 = client.responses.parse(
    model="gpt-4.1",
    input= f"**List of users under given transformer and their usage**{md_map}",
    text_format=UserAction,
    instructions = final_bot
)
actions = resp3.output_parsed
print(actions.user)
print(actions.der)
print(actions.message_type)
print(actions.reasoning)



with open("/content/ders.json", "r") as f:
    ders_data = json.load(f)

# Build der_id_map: (user, appliance) -> der_id
der_id_map = {}
for entry in ders_data["data"]:
    try:
        user = entry["attributes"]["energy_resource"]["data"]["attributes"]["name"]
        appliance = entry["attributes"]["appliance"]["data"]["attributes"]["name"]
        der_id = entry["id"]
        der_id_map[(user, appliance)] = der_id
    except Exception as e:
        continue

import random

def simulate_der_compliance(user_action_list, der_id_map):
    """
    user_action_list = [
        {"user": "Alice", "ders": ["Microwave", "Heater"], "message_type": "request"},
        ...
    ]

    der_id_map = {
        ("Alice", "Microwave"): 2532,
        ...
    }

    Returns a list of accepted actions with DER IDs
    """
    result_payload = []

    for row in user_action_list:
        for appliance in row["ders"]:
            key = (row["user"], appliance)
            der_id = der_id_map.get(key)
            if der_id is not None:
                # Apply compliance probability
                prob = 1.0 if row["message_type"] == "command" else (0.7 if row["message_type"] == "request" else 0.3)
                accepted = random.random() < prob
                if accepted:
                    result_payload.append({
                        "user": row["user"],
                        "appliance": appliance,
                        "message_type": row["message_type"],
                        "compliance": True,
                        "der_id": der_id
                    })

    return result_payload

def switch_appliance_state(der_id, on, token):
    """
    der_id: int ‚Äî ID of the DER in Strapi
    on: bool ‚Äî True to switch on, False to switch off
    token: str ‚Äî Bearer token for Strapi
    """
    url = f"https://world-engine-team6.becknprotocol.io/api/ders/{der_id}"
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json"
    }
    payload = {
        "data": {
            "switched_on": on
        }
    }

    try:
        res = requests.put(url, headers=headers, json=payload)
        return res.status_code, res.json()
    except Exception as e:
        return 500, {"error": str(e)}

# Step-by-step function to run full loop: display ‚Üí act ‚Üí display ‚Üí reset ‚Üí display
def full_shedding_loop(df, accepted_actions, token):
    import pandas as pd
    from time import sleep

    logs = ""

    # Helper to compute energy per user
    def summarize_user_energy(sub_df):
        summary = sub_df.groupby('User')['Base kWh Usage'].sum().reset_index()
        return summary

    # Step 1: Display original energy use
    users_of_interest = {entry["user"] for entry in accepted_actions}
    original_df = df[df["User"].isin(users_of_interest)]
    original_summary = summarize_user_energy(original_df)

    logs += "## üîç Original Total Energy (kWh) by User\n"
    logs += original_summary.to_markdown(index=False) + "\n\n"

    # Step 2: Turn OFF DERs
    for entry in accepted_actions:
        switch_appliance_state(entry["der_id"], on=False, token=token)

    sleep(1)  # simulate wait time

    # Step 3: Show updated energy after turning off
    off_appliances = {(entry["user"], entry["appliance"]) for entry in accepted_actions}
    updated_df = original_df[~original_df.apply(lambda row: (row["User"], row["Appliance"]) in off_appliances, axis=1)]
    updated_summary = summarize_user_energy(updated_df)

    logs += "## üîª After Shedding ‚Äî Remaining Energy (kWh)\n"
    logs += updated_summary.to_markdown(index=False) + "\n\n"

    # Step 4: Reset all appliances to ON
    for entry in accepted_actions:
        switch_appliance_state(entry["der_id"], on=True, token=token)

    sleep(1)

    # Step 5: Show restored energy
    restored_summary = summarize_user_energy(original_df)
    logs += "## üîÑ Restored ‚Äî Energy Back to Original\n"
    logs += restored_summary.to_markdown(index=False) + "\n"

    return logs

def accepted_actions_to_markdown(accepted_actions):
    """
    Convert list of accepted DER actions to markdown table.
    Each action should be a dict with keys:
    - user
    - appliance
    - message_type
    - der_id
    """
    df = pd.DataFrame(accepted_actions)
    df = df[["user", "appliance", "message_type", "der_id"]]
    df.columns = ["User", "Appliance", "Message Type", "DER ID"]
    return df.to_markdown(index=False)

def format_llm_output_to_actions(llm_output):
    user_action_list = []
    for user, ders, msg in zip(llm_output["user"], llm_output["der"], llm_output["message_type"]):
        if ders:  # ignore empty lists (like 'inform' types)
            user_action_list.append({
                "user": user,
                "ders": ders,
                "message_type": msg
            })
    return user_action_list
user_action_raw = json.loads(resp3.output_text)
user_action_list = format_llm_output_to_actions(user_action_raw)
accepted_actions = simulate_der_compliance(user_action_list,der_id_map)
print(accepted_actions)
STRAPI_TOKEN = API_TOKEN
log_output = full_shedding_loop(df, accepted_actions, STRAPI_TOKEN)
print(log_output)

def assign_rewards(full_action_list, accepted_actions):
    accepted_set = {(a["user"], a["appliance"]) for a in accepted_actions}
    rewards_log = []

    for action in full_action_list:
        user = action["user"]
        for appliance, msg_type in zip(action["ders"], [action["message_type"]]*len(action["ders"])):
            complied = (user, appliance) in accepted_set

            if msg_type == "suggestion":
                reward = 5 if complied else -2
            elif msg_type == "request":
                reward = 2 if complied else -1
            elif msg_type == "command":
                reward = 0 if complied else -10
            else:
                reward = 0  # fallback

            rewards_log.append({
                "User": user,
                "appliance": appliance,
                "message_type": msg_type,
                "complied": complied,
                "Reward": reward
            })

    return rewards_log

def rewards_to_markdown(rewards_log):
    df = pd.DataFrame(rewards_log)
    return df.to_markdown(index=False)

def aggregate_rewards(rewards_log):
    df = pd.DataFrame(rewards_log)
    summary = df.groupby("User")["Reward"].sum().reset_index()
    summary.columns = ["User", "Total Reward"]
    return summary.to_markdown(index=False)

window_summary_log = []  # global log to append to

def store_rl_window_summary(
    i,
    forecast_row,
    escalated,
    rl_message,
    transformer_ids,
    tx_map,
    shed_log,
    user_action_list,
    accepted_actions,
    total_energy_before,
    total_energy_after,
    total_reward
):
    summary = {
        "Window": i + 1,
        "Forecast": forecast_row,
        "Escalated": escalated,
        "Message to RL": rl_message,
        "Transformers Selected": transformer_ids,
        "TX Map": tx_map,
        "User Actions": user_action_list,
        "Accepted Actions": accepted_actions,
        "Energy Before": round(total_energy_before, 3),
        "Energy After": round(total_energy_after, 3),
        "Energy Delta": round(total_energy_before - total_energy_after, 3),
        "Total Reward": total_reward,
        "shed_log":shed_log
    }
    window_summary_log.append(summary)
def summarize_rl_experience(window_summary_log):
    md = "# üß† RL Experience Summary ‚Äî Last 6 Windows\n"
    for entry in window_summary_log:
        md += f"\n## üîÑ Window {entry['Window']}\n"
        md += f"- **Escalated?**: `{entry['Escalated']}`\n"
        md += f"- **Message to RL**: {entry['Message to RL']}\n"
        md += f"- **Transformers Selected**: {', '.join(entry['Transformers Selected'])}\n"
        md += f"- **Energy Before**: {entry['Energy Before']} kWh\n"
        md += f"- **Energy After**: {entry['Energy After']} kWh\n"
        md += f"-  **Log of energy before and after**:{entry['shed_log']}\n"
        md += f"- **Energy Reduction**: `{entry['Energy Delta']} kWh`\n"
        md += f"- **Total Reward**: {entry['Total Reward']}\n"




        md += f"- **TX ‚Üí User Map**:\n"
        for tx, users in entry["TX Map"].items():
            for user, appliances in users.items():
                appliance_names = [a if isinstance(a, str) else a.get("name", str(a)) for a in appliances]
                md += f"  - {tx} ‚Üí {user}: {', '.join(appliance_names)}\n"


        md += f"- **User Targeting Plan**:\n"
        for action in entry["User Actions"]:
            md += f"  - {action['user']}: {action['message_type']} ‚Üí {', '.join(action['ders'])}\n"

        md += f"- **DERs That Complied**:\n"
        for acc in entry["Accepted Actions"]:
            md += f"  - {acc['user']} ‚Üí {acc['appliance']} ‚úÖ\n"
    return md

def format_reflection(reflections):
    md = "## üß† Model Reflections Based on Past Conversations\n"
    md += "**The following internal analysis is generated based on observed actions, outcomes, and decision-making across previous RL windows. It may be used to refine future policy choices or fine-tune escalation thresholds.**\n\n"
    md += "### üîç Self-Evaluation Summary:\n"
    for idx, line in enumerate(reflections, 1):
        md += f"**{idx}.** {line.strip()}\n\n"
    return md

# Modified function: accumulate logs and yield each time (without overwriting)
window_summary_log = []
def run_simulation_live_accumulated():
    forecast_rows = formatted_df.head(6).to_dict(orient="records")
    log = ""

    for i, row in enumerate(forecast_rows):
        row_md = pd.DataFrame([row]).to_markdown(index=False)
        log += f"## üß† Forecast Evaluation ‚Äî Window {i+1}\n"
        log += f"```markdown\n{row_md}\n```\n"
        yield log

        eval_resp = client.responses.parse(
            model="gpt-4.1",
            input=[{"role": "user", "content": row_md}],
            text_format=ForecastEval,
            instructions=forecast_prompt
        )
        parsed_eval = eval_resp.output_parsed

        log += f"**Step 1 ‚Äî Decision to escalate:** `{parsed_eval.call_rl_agent}`  \n"
        log += f"**Step 1 ‚Äî Reasoning:** {parsed_eval.reasoning_trace}  \n"
        yield log

        if not parsed_eval.call_rl_agent:
            log += f"‚ö™ No RL escalation needed for window {i+1}.\n\n"
            yield log
            continue

        log += "\n---\n## üîÅ Step 2 ‚Äî Calling RL Agent\n"
        log += f"**Step 2 ‚Äî Message to RL**: {parsed_eval.message_to_rl}  \n"
        yield log

        rl_resp = client.responses.create(
            model="gpt-4.1",
            input=[{
                "role": "user",
                "content": f"**Message:** {parsed_eval.message_to_rl}\n\n**Reasoning:** {parsed_eval.reasoning_trace}\n\n**Transformer forecast map**{row_md}"
            }],
            tools=[{
                "type": "function",
                "name": "get_tx_appliance_map",
                "description": "Returns users and appliances for transformer IDs.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "transformer_ids": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "List of transformer IDs"
                        }
                    },
                    "required": ["transformer_ids"]
                }
            }],
            instructions=rl_prompt,
            tool_choice={"type": "function", "name": "get_tx_appliance_map"}
        )

        call = rl_resp.output[0]
        args = json.loads(call.arguments)
        #transformer_map = {str(i): f"Transformer_{i}" for i in range(1, 11)}
        #transformer_ids = [transformer_map[v] for v in args["transformer_ids"]]
        transformer_ids = args["transformer_ids"]
        tx_map = get_tx_appliance_map(transformer_ids)

        md_map = format_tx_map_md(tx_map)
        log += "\n---\n## üß≠ Step 3 ‚Äî User-DER Map\n"
        log += f"```markdown\n{md_map}\n```\n"
        yield log

        final_resp = client.responses.parse(
            model="gpt-4.1",
            input=[{"role": "user", "content": f"**User DER Map**:\n{md_map}"}],
            text_format=UserAction,
            instructions=final_bot
        )
        actions = final_resp.output_parsed

        log += "\n---\n## üì¶ Step 4 ‚Äî User Actions to Issue\n"
        table = pd.DataFrame({
            "User": actions.user,
            "DERs": ["; ".join(d) for d in actions.der],
            "Message Type": actions.message_type
        }).to_markdown(index=False)
        log += f"```markdown\n{table}\n```\n"
        log += f"**Step 4 ‚Äî Agent Reasoning**: {actions.reasoning}\n\n"
        yield log
user_action_raw = json.loads(resp3.output_text)
user_action_list = format_llm_output_to_actions(user_action_raw)
accepted_actions = simulate_der_compliance(user_action_list,der_id_map)
print(accepted_actions)
STRAPI_TOKEN = API_TOKEN
log_output = full_shedding_loop(df, accepted_actions, STRAPI_TOKEN)
print(log_output)

def run_simulation_live_accumulated_with_der_control():
    forecast_rows = formatted_df.head(6).to_dict(orient="records")
    log = ""

    for i, row in enumerate(forecast_rows):
        row_md = pd.DataFrame([row]).to_markdown(index=False)
        log += f"## üß† Forecast Evaluation ‚Äî Window {i+1}\n"
        log += f"```markdown\n{row_md}\n```\n"
        yield log

        # Forecast evaluation
        eval_resp = client.responses.parse(
            model="gpt-4.1",
            input=[{"role": "user", "content": row_md}],
            text_format=ForecastEval,
            instructions=forecast_prompt
        )
        parsed_eval = eval_resp.output_parsed

        log += f"**Step 1 ‚Äî Decision to escalate:** `{parsed_eval.call_rl_agent}`  \n"
        log += f"**Step 1 ‚Äî Reasoning:** {parsed_eval.reasoning_trace}  \n"
        yield log

        if not parsed_eval.call_rl_agent:
            log += f"‚ö™ No RL escalation needed for window {i+1}.\n\n"
            yield log
            continue

        log += "\n---\n## üîÅ Step 2 ‚Äî Calling RL Agent\n"
        log += f"**Step 2 ‚Äî Message to RL**: {parsed_eval.message_to_rl}  \n"
        yield log

        # Tool call for user-DER mapping
        rl_resp = client.responses.create(
            model="gpt-4.1",
            input=[{
                "role": "user",
                "content": f"**Message:** {parsed_eval.message_to_rl}\n\n**Reasoning:** {parsed_eval.reasoning_trace}\n\n**Transformer forecast map**{row_md}"
            }],
            tools=[{
                "type": "function",
                "name": "get_tx_appliance_map",
                "description": "Returns users and appliances for transformer IDs.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "transformer_ids": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "List of transformer IDs"
                        }
                    },
                    "required": ["transformer_ids"]
                }
            }],
            instructions=rl_prompt,
            tool_choice={"type": "function", "name": "get_tx_appliance_map"}
        )

        call = rl_resp.output[0]
        args = json.loads(call.arguments)
        transformer_ids = args["transformer_ids"]
        tx_map = get_tx_appliance_map(transformer_ids)

        md_map = format_tx_map_md(tx_map)
        log += "\n---\n## üß≠ Step 3 ‚Äî User-DER Map\n"
        log += f"```markdown\n{md_map}\n```\n"
        yield log

        # Final RL action planner
        final_resp = client.responses.parse(
            model="gpt-4.1",
            input=[{"role": "user", "content": f"**User DER Map**:\n{md_map}"}],
            text_format=UserAction,
            instructions=final_bot
        )
        actions = final_resp.output_parsed

        log += "\n---\n## üì¶ Step 4 ‚Äî User Actions to Issue\n"
        table = pd.DataFrame({
            "User": actions.user,
            "DERs": ["; ".join(d) for d in actions.der],
            "Message Type": actions.message_type
        }).to_markdown(index=False)
        log += f"```markdown\n{table}\n```\n"
        log += f"**Step 4 ‚Äî Agent Reasoning**: {actions.reasoning}\n\n"
        yield log

        # Step 5 ‚Äî Simulate compliance + execute DER control
        user_action_raw = {
            "user": actions.user,
            "der": actions.der,
            "message_type": actions.message_type
        }

        user_action_list = format_llm_output_to_actions(user_action_raw)
        accepted_actions = simulate_der_compliance(user_action_list, der_id_map)
        log += "### ‚úÖ Accepted DER Shedding Actions üîß\n"
        log += accepted_actions_to_markdown(accepted_actions) + "\n\n"
        yield log

        # Step 6 ‚Äî Full DER shed + reset loop
        shed_log = full_shedding_loop(df, accepted_actions, API_TOKEN)
        log += shed_log + "\n"
            #log+= f"```markdown\n{shed_log}\n```\n"

        rewards_log = assign_rewards(user_action_list, accepted_actions)

        log += "## üèÖ Appliance-Level Rewards\n"
        log += rewards_to_markdown(rewards_log) + "\n\n"
        yield log

        log += "## üßÆ Total Reward per User\n"
        log += aggregate_rewards(rewards_log) + "\n"
        yield log
        yield log

def run_simulation_live_accumulated_with_der_control():
    forecast_rows = formatted_df.head(2).to_dict(orient="records")
    log = ""

    for i, row in enumerate(forecast_rows):
        row_md = pd.DataFrame([row]).to_markdown(index=False)
        log += f"## üß† Forecast Evaluation ‚Äî Window {i+1}\n"
        log += f"{row_md}\n\n"
        yield log

        # Step 1 ‚Äî Forecast evaluation
        eval_resp = client.responses.parse(
            model="gpt-4.1",
            input=[{"role": "user", "content": row_md}],
            text_format=ForecastEval,
            instructions=forecast_prompt
        )
        parsed_eval = eval_resp.output_parsed

        log += f"**Step 1 ‚Äî Decision to escalate:** `{parsed_eval.call_rl_agent}`  \n"
        log += f"**Step 1 ‚Äî Reasoning:** {parsed_eval.reasoning_trace}  \n"
        yield log

        if not parsed_eval.call_rl_agent:
            log += f"‚ö™ No RL escalation needed for window {i+1}.\n\n"
            yield log
            continue

        # Step 2 ‚Äî Tool call
        log += "\n---\n## üîÅ Step 2 ‚Äî Calling RL Agent\n"
        log += f"**Step 2 ‚Äî Message to RL**: {parsed_eval.message_to_rl}  \n"
        yield log

        rl_resp = client.responses.create(
            model="gpt-4.1",
            input=[{
                "role": "user",
                "content": f"**Message:** {parsed_eval.message_to_rl}\n\n**Reasoning:** {parsed_eval.reasoning_trace}\n\n**Transformer forecast map**{row_md}"
            }],
            tools=[{
                "type": "function",
                "name": "get_tx_appliance_map",
                "description": "Returns users and appliances for transformer IDs.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "transformer_ids": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "List of transformer IDs"
                        }
                    },
                    "required": ["transformer_ids"]
                }
            }],
            instructions=rl_prompt,
            tool_choice={"type": "function", "name": "get_tx_appliance_map"}
        )

        call = rl_resp.output[0]
        args = json.loads(call.arguments)
        transformer_ids = args["transformer_ids"]
        tx_map = get_tx_appliance_map(transformer_ids)

        # Step 3 ‚Äî User-DER map
        md_map = format_tx_map_md(tx_map)
        log += "\n---\n## üß≠ Step 3 ‚Äî User-DER Map\n"
        log += f"{md_map}\n\n"
        yield log

        # Step 4 ‚Äî Final RL planner
        final_resp = client.responses.parse(
            model="gpt-4.1",
            input=[{"role": "user", "content": f"**User DER Map**:\n{md_map}"}],
            text_format=UserAction,
            instructions=final_bot
        )
        actions = final_resp.output_parsed

        table = pd.DataFrame({
            "User": actions.user,
            "DERs": ["; ".join(d) for d in actions.der],
            "Message Type": actions.message_type
        }).to_markdown(index=False)

        log += "\n---\n## üì¶ Step 4 ‚Äî User Actions to Issue\n"
        log += f"{table}\n"
        log += f"**Step 4 ‚Äî Agent Reasoning**: {actions.reasoning}\n\n"
        yield log

        # Step 5 ‚Äî Compliance simulation
        user_action_raw = {
            "user": actions.user,
            "der": actions.der,
            "message_type": actions.message_type
        }

        user_action_list = format_llm_output_to_actions(user_action_raw)
        accepted_actions = simulate_der_compliance(user_action_list, der_id_map)

        log += "### ‚úÖ Accepted DER Shedding Actions üîß\n"
        log += accepted_actions_to_markdown(accepted_actions) + "\n\n"
        yield log

        # Step 6 ‚Äî DER switching + tracking energy
        total_energy_before = df[df["User"].isin([a["user"] for a in accepted_actions])]["Base kWh Usage"].sum()

        shed_log = full_shedding_loop(df, accepted_actions, API_TOKEN)
        log += shed_log + "\n"

        total_energy_after = df[df["User"].isin([a["user"] for a in accepted_actions])]["Base kWh Usage"].sum()

        rewards_log = assign_rewards(user_action_list, accepted_actions)
        total_reward = sum([r["Reward"] for r in rewards_log])

        log += "## üèÖ Appliance-Level Rewards\n"
        log += rewards_to_markdown(rewards_log) + "\n\n"
        yield log

        log += "## üßÆ Total Reward per User\n"
        log += aggregate_rewards(rewards_log) + "\n"
        yield log



        # ‚úÖ Store full experience for meta-evaluation
        store_rl_window_summary(
            i=i,
            forecast_row=row,
            escalated=parsed_eval.call_rl_agent,
            rl_message=parsed_eval.message_to_rl,
            transformer_ids=transformer_ids,
            tx_map=tx_map,
            user_action_list=user_action_list,
            accepted_actions=accepted_actions,
            shed_log = shed_log,
            total_energy_before=total_energy_before,
            total_energy_after=total_energy_after,
            total_reward=total_reward,
           )

    # Step 7 ‚Äî At the end of all 6 windows
    log += "## üß† RL Self-Reflection Summary (All Windows)\n"
    log += summarize_rl_experience(window_summary_log) + "\n"

    yield log

    reflection_resp = client.responses.parse(
            model="gpt-4.1",
            input=f"**User DER Map**:\n{md_map}\n **logs:**\n{ summarize_rl_experience(window_summary_log)}",
            text_format=UserReflection,
            instructions=reflection_bot
        )
    reflect = reflection_resp.output_parsed.reflection

    log += format_reflection(reflect)
    yield log

# Re-import gradio after environment reset
import gradio as gr

# Plug in the simulation function into Gradio interface
with gr.Blocks() as demo:
    gr.Markdown("# üîÑ Grid Forecast + Load Shedding Simulator")
    gr.Markdown("Simulates 6 rounds of forecast evaluation, RL escalation, DER shedding decisions with step-by-step outputs.")

    run_button = gr.Button("‚ñ∂Ô∏è Run Simulation")
    result_box = gr.Markdown()

    run_button.click(fn=run_simulation_live_accumulated_with_der_control, outputs=result_box)

demo.launch(debug=True)

import gradio as gr
import pandas as pd
import json
import folium
from folium.plugins import MarkerCluster

# === LOAD JSON ===
def load_json(path):
    with open(path) as f:
        return json.load(f)["data"]

meters = load_json("/content/meters.json")
ders = load_json("/content/ders.json")

# === MAP GENERATOR ===
def generate_map():
    m = folium.Map(location=[12.9716, 77.5946], zoom_start=12)
    cluster = MarkerCluster().add_to(m)
    for meter in meters:
        attr = meter["attributes"]
        lat, lon = attr.get("latitude"), attr.get("longitude")
        user = attr.get("user_name", "Unknown")
        if lat and lon:
            folium.Marker(
                location=[lat, lon],
                popup=f"<b>{user}</b>",
                icon=folium.Icon(color="blue")
            ).add_to(cluster)
    return m._repr_html_()

# === BUILD USER ‚Üí DER MAP ===
def build_user_der_map(ders):
    user_map = {}
    for der in ders:
        attr = der["attributes"]
        user = attr.get("user_name") or "Unknown"
        name = attr.get("name", "Unnamed DER")
        der_type = attr.get("type", "unknown")
        if user not in user_map:
            user_map[user] = []
        user_map[user].append((name, der_type))
    return user_map

user_der_map = build_user_der_map(ders)
user_list = sorted(user_der_map.keys())

# === SHOW USER DER INFO ===
def show_user_ders(user):
    ders = user_der_map.get(user, [])
    if not ders:
        return f"üîç No DERs found for **{user}**"
    df = pd.DataFrame(ders, columns=["DER", "Type"])
    return f"### DERs for {user}\n\n" + df.to_markdown(index=False)

# === Dummy simulation logic for test ===
def run_simulation_live_accumulated_with_der_control():
    return "‚úÖ Simulation executed. (This is placeholder text.)"

# === Reset handler ===
def reset_simulation():
    return "üîÑ Simulation reset."

# === GRADIO UI ===
with gr.Blocks() as demo:
    gr.Markdown("# ‚ö° Grid Forecast + DER Control Simulator")

    with gr.Row():
        run_btn = gr.Button("‚ñ∂Ô∏è Run Simulation")
        reset_btn = gr.Button("üîÑ Reset")
        map_btn = gr.Button("üó∫Ô∏è Show Grid Map")
        user_btn = gr.Button("üë• User Dashboard")

    output_box = gr.Markdown()

    with gr.Row():
        map_box = gr.HTML(visible=False)
        user_dropdown = gr.Dropdown(user_list, label="Select a User", visible=False)
        user_info = gr.Markdown(visible=False)

    run_btn.click(run_simulation_live_accumulated_with_der_control, outputs=output_box)
    reset_btn.click(reset_simulation, outputs=output_box)

    map_btn.click(fn=generate_map, outputs=map_box)
    map_btn.click(lambda: gr.update(visible=True), outputs=map_box)

    user_btn.click(lambda: gr.update(visible=True), outputs=user_dropdown)
    user_dropdown.change(fn=show_user_ders, inputs=user_dropdown, outputs=user_info)
    user_dropdown.change(lambda: gr.update(visible=True), outputs=user_info)

demo.launch(debug=True)